[project]
name = "blossomtune"
version = "1.0.0"
description = "BlossomTune LLM"
license = "Apache-2.0"
dependencies = [
    "flwr[simulation]>=1.19.0",
    "flwr-datasets>=0.5.0",
    "torch==2.8.0",
    "trl==0.19.0",
    "bitsandbytes==0.46.0 ; platform_system != 'Darwin'",
    "scipy==1.16.0",
    "peft==0.15.2",
    "transformers==4.53.0",
    "sentencepiece==0.2.0",
    "omegaconf==2.3.0",
    "hf_transfer==0.1.9",
    "torch-adopt==0.1.0",
    "python-slugify>=8.0.4",
    "jinja2>=3.1.6",
]
requires-python = ">=3.11.12"

[tool.ruff.lint]

# Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`)  codes by default.
select = ["E4", "E7", "E9", "F"]
ignore = []

# Allow fix for all enabled rules (when `--fix`) is provided.
fixable = ["ALL"]
unfixable = []

[tool.ruff.format]

# Double quotes for strings.
quote-style = "double"

# Iindent with spaces.
indent-style = "space"

# Respect magic trailing commas.
skip-magic-trailing-comma = false

# Automatically detect line ending.
line-ending = "auto"

# Disable auto-formatting of code examples in docstrings.
docstring-code-format = false

# Set the line length limit used when formatting code snippets in
# docstrings.
#
# This only has an effect when the `docstring-code-format` setting is
# enabled.
docstring-code-line-length = "dynamic"

[tool.ruff.lint.pydocstyle]
convention = "google"  # Accepts: "google", "numpy", or "pep257".

[tool.flwr.app]
publisher = "mrs83"

[tool.flwr.app.components]
serverapp = "blossomtune.server_app:app"
clientapp = "blossomtune.client_app:app"

[tool.flwr.app.config]
model.name = "HuggingFaceTB/SmolLM2-135M-Instruct"
model.quantization = 4
model.gradient-checkpointing = true
model.lora.peft-lora-r = 8
model.lora.peft-lora-alpha = 16
model.lora.peft-use-dora = true
model.lora.peft-target-modules = "q_proj,k_proj,v_proj,o_proj,gate_proj,down_proj,up_proj"
model.use-adopt = true
train.save-every-round = 5
train.learning-rate-max = 5e-5
train.learning-rate-min = 1e-6
train.seq-length = 512
train.training-arguments.output-dir = ""
train.training-arguments.learning-rate = ""
train.training-arguments.per-device-train-batch-size = 8
train.training-arguments.gradient-accumulation-steps = 1
train.training-arguments.logging-steps = 1
train.training-arguments.num-train-epochs = 3
train.training-arguments.max-steps = 8
train.training-arguments.save-steps = 1000
train.training-arguments.save-total-limit = 10
train.training-arguments.max-grad-norm = 1.0
train.training-arguments.gradient-checkpointing = true
train.training-arguments.bf16 = true
train.training-arguments.tf32 = true
train.training-arguments.optim = "adamw_torch"
train.training-arguments.eos_token = "<|im_end|>"
train.training-arguments.lr-scheduler-type = "constant"
strategy.fraction-fit = 1.0
strategy.fraction-evaluate = 0.0
num-server-rounds = 20
use-flexlora = false
data-path = "/app/data"
save-path = "/app/results"
dataset.name = "flwrlabs/alpaca-gpt4"
dataset.prompt_template = "{% if instruction %}{{ instruction }} {% endif %}{{ input }}"
dataset.completion_template = "{{output}}"

[tool.flwr.federations]
default = "local-deployment"

[tool.flwr.federations.local-deployment]
address = "0.0.0.0:9093"
insecure = true

[tool.flwr.federations.local-simulation]
options.num-supernodes = 10
options.backend.client-resources.num-cpus = 4
options.backend.client-resources.num-gpus = 1.0

[dependency-groups]
dev = [
    "pytest>=8.4.1",
]

[tool.pytest.ini_options]
pythonpath = "."
addopts = [
    "--import-mode=importlib",
]
